---
title: "M-Bias: Confounding Control Using Three Waves of Panel Data"
format: html
title-block-style: plain
bibliography: /Users/joseph/GIT/b-causal/posts/m-bias/references.bib
date: 2022-11-22
date-format: short
image: "m-bias.png"
author: 
  - name: Joseph Bulbulia
    orcid: 0000-0002-5861-2056
    affiliation: Victoria University of Wellington, New Zealand
    email: joseph.bulbulia@vuw.ac.nz
    corresponding: yes
keep-tex: false
citation:
  url: https://go-bayes.github.io/b-causal/
execute:
  echo: false
  warning: false
  message: false
  error: false
categories:
  - Causal Inference
  - Outcome-wide Science
  - Methods
---



```{r}
#| include: false
#| echo: false
#read libraries

library("tinytex")
library(extrafont)
loadfonts(device = "all")

# read libraries
source("/Users/joseph/GIT/templates/functions/libs2.R")

# read functions
#source("/Users/joseph/GIT/templates/functions/funs.R")
```



## Review

[Elsewhere](https://go-bayes.github.io/b-causal/posts/outcomewide/outcome-wide.html), we have described our strategy for using three waves of panel data to identify causal effects. For confounding control, we adopt VanderWeele's modified disjunctive cause criterion:

> control for each covariate that is a cause of the exposure, or of the outcome, or of both; exclude from this set any variable known to be an instrumental variable; and include as a covariate any proxy for an unmeasured variable that is a common cause of both the exposure and the outcome [@vanderweele2020 p.441; @vanderweele2019].

Such a criterion might appear to be too liberal. It might seem that we should instead select the minimum adjustment set of confounders necessary for confounding control. Of course, the minimum adjustment set cannot generally be known. However, a liberal inclusion criterion would seem to invite confounding by over-conditioning. We next consider the risks of such liberality in three-wave panel designs.

## M-bias

M-bias is a form of bias that can arise when we include too many variables in our analysis, a phenomenon known as over-conditioning. Let's break this down using a concrete example. 

Suppose we're interested in understanding if being a perfectionist influences a person's level of humility. We start with the assumption that there's no direct cause-and-effect relationship between perfectionism (the exposure) and humility (the outcome).

Now imagine we're including forgiveness in our analysis. We know that childhood schooling influences both forgiveness and perfectionism, and childhood religion affects forgiveness and humility. If we adjust for forgiveness in our analysis, an indirect path (or backdoor path) is created between perfectionism and humility, leading to M-bias. This path can be illustrated as @fig-1.


```{tikz} 
#| label: fig-1
#| fig-cap: "M-bias: an example of confounding that arises from over-adjustment"
#| out-width: 100%
#| echo: false


\usetikzlibrary{positioning}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{arrows}
\usetikzlibrary{decorations}
\tikzstyle{Arrow} = [->, thin, preaction = {decorate}]
\tikzstyle{DoubleArrow} = [-, preaction = {decorate}]
\tikzset{>=latex}

\begin{tikzpicture}[{every node/.append style}=draw]

\node [rectangle, draw=white] (U1) at (0, 2) {Childhood~Schooling};
\node [rectangle, draw=white] (U2) at (0, -2) {Childhood~Religion};
\node [rectangle, draw=black] (L) at (4, 0) {Forgiveness};
\node [rectangle, draw=white] (A) at (8, 0) {Perfectionism};
\node [rectangle, draw=white] (Y) at (12, 0) {Humility};

\draw [-latex, draw=black] (U1) to (L);
\draw [-latex, draw =black] (U2) to (L);
\draw [-latex, draw=black, bend left] (U1) to (Y);
\draw [-latex, draw =black, bend right] (U2) to (Y);
\draw [-latex,  draw=red, red, dashed] (A) to (Y);

\end{tikzpicture}
```

By including forgiveness in our model, we've inadvertently introduced a correlation between perfectionism and humility where one didn't previously exist. This is the essence of M-bias.

## The Power of Baseline Measures


One might think the solution is simple - don't include forgiveness in the model. However, our understanding of causal relationships is often imperfect, and there may be plausible reasons to believe that forgiveness does, in fact, influence perfectionism.  If this were the case, to avoid bias, we would need to condition on a proxy of the unmeasured variables. This strategy might not completely avoid bias, which is why it is important to include sensitivity analyses to assess how much unmeasured confounding would be required to explain a result away [@vanderweele2017]


## Conclusion

In our pursuit of understanding causal relationships, we must carefully navigate the risk of M-biasâ€”a form of confounding that can emerge from over-adjusting for variables. We've outlined a strategy to mitigate this bias by including both prior measurements of the exposure and the outcome in our studies. This approach provides a robust mechanism to control for unmeasured confounders that might otherwise skew our results. However, even with these measures, we cannot guarantee the elimination of all confounding. For this reason, we also conduct sensitivity analysis using E-values to assess the robustness of our findings to potential unmeasured confounding. E-values provide a quantitative measure of the minimum strength an unmeasured confounder would need to fully explain away an observed association. 

In future posts, we will delve more deeply into the concept of E-values and their role in robust causal inference. By leveraging strategies for confounding control that include previous measures of the outcome and exposure variables, as well as senstivity analysis, we strive for more reliable, accurate insights in our studies.



## Acknowledgements

I am grateful to Templeton Religion Trust Grant 0418 for supporting my work.[^2]

[^2]: The funders played no role in the design or interpretation of this research.
